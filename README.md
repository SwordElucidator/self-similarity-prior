<<<<<<< HEAD
# Self-Similarity Priors: Neural Collages as Differentiable Fractal Representations
=======
# Neural Collages as Differentiable Fractal Representations
>>>>>>> 0aea193784c497ac530054bab0daa2941d45dcbb

<p align="center">
<video src="media/CollageSteps.mp4">
</p>
<div align="center">
      
<<<<<<< HEAD

[![License](https://img.shields.io/badge/License-MIT-black.svg?)](https://papers.nips.cc/paper/2020/hash/f1686b4badcf28d33ed632036c7ab0b8-Abstract.html)
=======
[![NeurIPS](https://img.shields.io/badge/NeurIPS-2022-red.svg?)]()
[![License](https://img.shields.io/badge/License-MIT-black.svg?)]()
>>>>>>> 0aea193784c497ac530054bab0daa2941d45dcbb
[![arXiv](https://img.shields.io/badge/arXiv-2204.07673-purple.svg?)](https://arxiv.org/abs/2204.07673)

</div>

<<<<<<< HEAD
=======
> Many patterns in nature exhibit self-similarity: they can be compactly described via self-referential transformations. Said patterns commonly appear in natural and artificial objects, such as molecules, shorelines, galaxies and even images. In this work, we investigate the role of learning in the automated discovery of self-similarity and in its utilization for downstream tasks. To this end, we design a novel class of implicit operators, Neural Collages, which (1) represent data as the parameters of a self-referential, structured transformation, and (2) employ hypernetworks to amortize the cost of finding these parameters to a single forward pass. We investigate how to leverage the representations produced by Neural Collages in various tasks, including data compression and generation. Neural Collages image compressors are orders of magnitude faster than other self-similarity-based algorithms during encoding and offer compression rates competitive with implicit methods. Finally, we showcase applications of Neural Collages for fractal art and as deep generative models.

>>>>>>> 0aea193784c497ac530054bab0daa2941d45dcbb
```
@article{poli2022self,
    title={Self-Similarity Priors: Neural Collages as Differentiable Fractal Representations},
    author={Poli, Michael and Xu, Winnie and Massaroli, Stefano and Meng, Chenlin and Kim, Kuno and Ermon, Stefano}, 
    journal={arXiv preprint arXiv:2204.07673}, 
    year={2022}
      }
```
<<<<<<< HEAD
=======


>>>>>>> 0aea193784c497ac530054bab0daa2941d45dcbb
